{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ywZ6aJyNoN7L"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-27 13:21:49.722621: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-27 13:21:49.722687: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-27 13:21:49.724087: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-27 13:21:49.855139: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "from os import environ, path\n",
        "from absl import logging as absl_logging\n",
        "from IPython.display import clear_output\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import ParameterSampler\n",
        "import os\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "# import keras_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u4it3O-QpmR8"
      },
      "outputs": [],
      "source": [
        "##Variables\n",
        "IMAGE_SIZE = 128\n",
        "BATCH_SIZE = 8\n",
        "NUM_CLASSES = 104\n",
        "LEARNING_RATE=0.002\n",
        "WEIGHT_DECAY=0.0001\n",
        "MOMENTUM=0.9\n",
        "CLIPNORM=10.0\n",
        "\n",
        "\n",
        "EPOCHS=25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fsy1-aEo-ah",
        "outputId": "25bae9f1-f730-44fe-d27f-2702517ec79f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models folder already exists\n"
          ]
        }
      ],
      "source": [
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "    isInColab = True\n",
        "else:\n",
        "    isInColab = False\n",
        "\n",
        "if isInColab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    data_folder_path = Path(os.getcwd() + r\"/gdrive/My Drive/FoodSeg103/Images\")\n",
        "    models_path = Path(os.getcwd() + r\"/gdrive/My Drive/Models\")\n",
        "else:\n",
        "    data_folder_path = Path(os.getcwd()+ r\"/Dataset/FoodSeg103/Images\")\n",
        "    models_path = Path(os.getcwd() + r\"/Models\")\n",
        "try:\n",
        "    os.mkdir(models_path)\n",
        "    print(\"Models folder created for saving models\")\n",
        "except:\n",
        "    print(\"Models folder already exists\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "f18zrsLPp2We"
      },
      "outputs": [],
      "source": [
        "##Data loading\n",
        "def load_images_combined(NUM_TRAIN_IMAGES, NUM_VAL_IMAGES):\n",
        "    train_images_path = os.path.join(data_folder_path, r\"img_dir/train\")\n",
        "    train_ann_path = os.path.join(data_folder_path, r\"ann_dir/train\")\n",
        "    test_images_path = os.path.join(data_folder_path, r\"img_dir/test\")\n",
        "    test_ann_path = os.path.join(data_folder_path, r\"ann_dir/test\")\n",
        "    train_images_paths = sorted(os.listdir(train_images_path))\n",
        "    train_ann_paths = sorted(os.listdir(train_ann_path))\n",
        "    test_images_paths = sorted(os.listdir(test_images_path))\n",
        "    test_ann_paths = sorted(os.listdir(test_ann_path))\n",
        "\n",
        "    train_images = train_images_paths[:NUM_TRAIN_IMAGES]\n",
        "    train_masks = train_ann_paths[:NUM_TRAIN_IMAGES]\n",
        "    val_images = test_images_paths[:NUM_VAL_IMAGES]\n",
        "    val_masks = test_ann_paths[:NUM_VAL_IMAGES]\n",
        "\n",
        "    train_images = [str(os.path.join(train_images_path, img)) for img in train_images]\n",
        "    train_masks = [str(os.path.join(train_ann_path, img)) for img in train_masks]\n",
        "    val_images = [str(os.path.join(test_images_path, img)) for img in val_images]\n",
        "    val_masks = [str(os.path.join(test_ann_path, img)) for img in val_masks]\n",
        "\n",
        "    image_paths=train_images+val_images\n",
        "    mask_paths=train_masks+val_masks\n",
        "    image_paths.sort()\n",
        "    mask_paths.sort()\n",
        "    # images={\n",
        "    #     \"image\": train_images+val_images,\n",
        "    #     \"masks\": train_masks+val_masks\n",
        "    # }\n",
        "    return image_paths, mask_paths\n",
        "\n",
        "image_paths, mask_paths = load_images_combined(4983, 2135)\n",
        "# dataset = tf.data.Dataset.from_tensor_slices((images['image'], images['masks']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "HZP_MO4vFrOY"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import Sequence\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class CustomDataGenerator(Sequence):\n",
        "    def __init__(self, image_paths, mask_paths, batch_size, validation_split=0.2):\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.batch_size = batch_size\n",
        "        self.validation_split = validation_split\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Split the data into training and validation sets\n",
        "        self.train_image_paths, self.val_image_paths, self.train_mask_paths, self.val_mask_paths = \\\n",
        "            train_test_split(self.image_paths, self.mask_paths, test_size=self.validation_split, random_state=42)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.train_image_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        start = index * self.batch_size\n",
        "        end = (index + 1) * self.batch_size\n",
        "        batch_x, batch_y = self.load_data(start, end)\n",
        "\n",
        "        # Optionally repeat the dataset\n",
        "        if end == len(self.train_image_paths):\n",
        "            self.on_epoch_end()  # Reset at the end of an epoch\n",
        "\n",
        "        return batch_x, batch_y\n",
        "        \n",
        "        # start = index * self.batch_size\n",
        "        # end = (index + 1) * self.batch_size\n",
        "        # batch_x, batch_y = self.load_data(start, end)\n",
        "        # return batch_x, batch_y\n",
        "\n",
        "    def load_data(self, start, end):\n",
        "        # Load and preprocess data\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "        paths = self.train_image_paths  # Use training paths during training\n",
        "        # max_pixel_value = 0\n",
        "        for i in range(start, min(end, len(self.image_paths))):\n",
        "            # Load and preprocess image\n",
        "            img_img = tf.io.read_file(self.image_paths[i])\n",
        "            img = tf.image.decode_png(img_img, channels=3)\n",
        "            img.set_shape([None, None, 3])\n",
        "            img = tf.image.resize(images=img, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "            img = tf.keras.applications.resnet50.preprocess_input(img)\n",
        "            batch_x.append(img)\n",
        "\n",
        "            # Load and preprocess mask\n",
        "            mask_img = tf.io.read_file(self.mask_paths[i])\n",
        "            mask = tf.image.decode_png(mask_img, channels=1)\n",
        "            mask.set_shape([None, None, 1])\n",
        "            mask = tf.image.resize(images=mask, size=[IMAGE_SIZE, IMAGE_SIZE], method='nearest')\n",
        "            one_hot_mask=to_categorical(mask,num_classes=NUM_CLASSES)\n",
        "            # max_value_in_image = np.max(mask)\n",
        "            # max_pixel_value = max(max_pixel_value, max_value_in_image)\n",
        "            batch_y.append(one_hot_mask)\n",
        "        # print(\"Overall Maximum Pixel Value:\", max_pixel_value)\n",
        "        return np.array(batch_x), np.array(batch_y)\n",
        "\n",
        "\n",
        "train_image_paths, val_image_paths, train_mask_paths, val_mask_paths = train_test_split(\n",
        "    image_paths, mask_paths, test_size=0.2)\n",
        "\n",
        "# Create instances of your custom data generator for training and validation\n",
        "train_data_generator = CustomDataGenerator(train_image_paths, train_mask_paths, batch_size=BATCH_SIZE)\n",
        "val_data_generator = CustomDataGenerator(val_image_paths, val_mask_paths,  batch_size=BATCH_SIZE)\n",
        "\n",
        "# data_generator = CustomDataGenerator(image_paths, mask_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "S3BRyPJPFrEw"
      },
      "outputs": [],
      "source": [
        "##The model\n",
        "def convolution_block(\n",
        "    block_input,\n",
        "    num_filters=256,\n",
        "    kernel_size=3,\n",
        "    dilation_rate=1,\n",
        "    padding=\"same\",\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=dilation_rate,\n",
        "        padding=\"same\",\n",
        "        use_bias=use_bias,\n",
        "        kernel_initializer=keras.initializers.HeNormal(),\n",
        "    )(block_input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def DilatedSpatialPyramidPooling(dspp_input):\n",
        "    dims = dspp_input.shape\n",
        "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
        "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
        "    out_pool = layers.UpSampling2D(\n",
        "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "\n",
        "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
        "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
        "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
        "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
        "    output = convolution_block(x, kernel_size=1)\n",
        "    return output\n",
        "\n",
        "def DeeplabV3Plus(image_size, num_classes):\n",
        "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
        "    resnet50 = keras.applications.ResNet50(\n",
        "        weights=\"imagenet\", include_top=False, input_tensor=model_input\n",
        "    )\n",
        "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
        "    x = DilatedSpatialPyramidPooling(x)\n",
        "\n",
        "    input_a = layers.UpSampling2D(\n",
        "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
        "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
        "    x = convolution_block(x)\n",
        "    x = convolution_block(x)\n",
        "    x = layers.UpSampling2D(\n",
        "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\", kernel_initializer=keras.initializers.HeNormal())(x)\n",
        "    # model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
        "    # Adjust the number of output channels to match the number of classes\n",
        "    x_out = layers.Conv2D(104, (1, 1), activation='softmax')(x)\n",
        "\n",
        "    # Use a Reshape layer to match the output shape to (height, width, num_classes)\n",
        "    x_out = layers.Reshape((image_size, image_size, 104))(x_out)\n",
        "\n",
        "\n",
        "    return keras.Model(inputs=model_input, outputs=x_out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5694\n",
            "1424\n",
            "711\n",
            "178\n",
            "711\n"
          ]
        }
      ],
      "source": [
        "print(len(train_image_paths))\n",
        "print(len(val_image_paths))\n",
        "\n",
        "1424 + 5694\n",
        "print(len(train_image_paths) // BATCH_SIZE)\n",
        "print(len(val_image_paths) // BATCH_SIZE)\n",
        "print(17775 // 25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAGi5Z2SO8Jw",
        "outputId": "b99c52f7-2911-4e58-e6b2-c6c91210f973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "570/711 [=======================>......] - ETA: 28s - loss: 2.1343 - one_hot_mean_io_u_1: 0.0258 - categorical_accuracy: 0.5350"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-27 13:35:35.582057: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12094384989605534130\n",
            "2023-11-27 13:35:35.582119: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17014484570836317922\n",
            "2023-11-27 13:35:35.582129: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10197571004204363545\n",
            "2023-11-27 13:35:35.582135: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14637221559000373045\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 17775 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 178 batches). You may need to use the repeat() function when building your dataset.\n",
            "711/711 [==============================] - 157s 198ms/step - loss: 2.1343 - one_hot_mean_io_u_1: 0.0258 - categorical_accuracy: 0.5350 - val_loss: 1.6944 - val_one_hot_mean_io_u_1: 0.0548 - val_categorical_accuracy: 0.5987\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-27 13:36:00.846802: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5633156238226856114\n",
            "2023-11-27 13:36:00.846864: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13243949958192383302\n",
            "2023-11-27 13:36:00.846871: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 140480356009411277\n",
            "2023-11-27 13:36:00.846875: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17241032318158015599\n"
          ]
        }
      ],
      "source": [
        "model=DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.SGD(learning_rate=0.002, weight_decay=0.0001, momentum=0.9, clipnorm=10.0),\n",
        "      loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "      metrics=[\n",
        "          keras.metrics.OneHotMeanIoU(num_classes=NUM_CLASSES, ignore_class=0),\n",
        "          keras.metrics.CategoricalAccuracy(),\n",
        "      ])\n",
        "history = model.fit(\n",
        "    train_data_generator,\n",
        "    steps_per_epoch=len(train_image_paths)//BATCH_SIZE,\n",
        "    # steps_per_epoch=500,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_data_generator,\n",
        "    validation_steps=len(val_image_paths)//BATCH_SIZE\n",
        "    # validation_steps=125\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "def saveModel(model, history):\n",
        "    dt_string = datetime.now().strftime(\"%d%m%Y-%H:%M:%S\")\n",
        "    save_name = \"model_\" + dt_string\n",
        "    folder_save_path = os.path.join(models_path, Path(save_name))\n",
        "    os.mkdir(folder_save_path)\n",
        "    his_save_path = os.path.join(folder_save_path, Path(r\"history.npy\"))\n",
        "    model_save_path = os.path.join(folder_save_path, Path(r\"model.keras\"))\n",
        "    np.save(his_save_path,history)\n",
        "    model.save(model_save_path)\n",
        "\n",
        "saveModel(model, history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bmj8XoYcO8HM"
      },
      "outputs": [],
      "source": [
        "class_labels={0:'background',1:'candy',2:'egg tart',3:'french fries',4:'chocolate',5:'biscuit',6:'popcorn',7:'pudding',8:'ice cream',9:'cheese butter',10:'cake',11:'wine',12:'milkshake',13:'coffee',14:'juice',15:'milk',16:'tea',17:'almond',18:'red beans',19:'cashew',20:'dried cranberries',21:'soy',22:'walnut',23:'peanut',24:'egg',25:'apple',26:'date',27:'apricot',28:'avocado',29:'banana',30:'strawberry',31:'cherry',32:'blueberry',33:'raspberry',34:'mango',35:'olives',36:'peach',37:'lemon',38:'pear',39:'fig',40:'pineapple',41:'grape',42:'kiwi',43:'melon',44:'orange',45:'watermelon',46:'steak',47:'pork',48:'chicken duck',49:'sausage',50:'fried meat',51:'lamb',52:'sauce',53:'crab',54:'fish',55:'shellfish',56:'shrimp',57:'soup',58:'bread',59:'corn',60:'hamburg',61:'pizza',62:' hanamaki baozi',63:'wonton dumplings',64:'pasta',65:'noodles',66:'rice',67:'pie',68:'tofu',69:'eggplant',70:'potato',71:'garlic',72:'cauliflower',73:'tomato',74:'kelp',75:'seaweed',76:'spring onion',77:'rape',78:'ginger',79:'okra',80:'lettuce',81:'pumpkin',82:'cucumber',83:'white radish',84:'carrot',85:'asparagus',86:'bamboo shoots',87:'broccoli',88:'celery stick',89:'cilantro mint',90:'snow peas',91:' cabbage',92:'bean sprouts',93:'onion',94:'pepper',95:'green beans',96:'French beans',97:'king oyster mushroom',98:'shiitake',99:'enoki mushroom',100:'oyster mushroom',101:'white button mushroom',102:'salad',103:'other ingredients'}\n",
        "\n",
        "def predict(model, images, index, useTrainingImages):\n",
        "    ImageIndex = index\n",
        "\n",
        "    if useTrainingImages:\n",
        "      img_path = images[\"train_images\"][ImageIndex]\n",
        "      ann_path = images[\"train_masks\"][ImageIndex]\n",
        "    else:\n",
        "      img_path = images[\"val_images\"][ImageIndex]\n",
        "      ann_path = images[\"val_masks\"][ImageIndex]\n",
        "\n",
        "    processed_image = read_image(img_path)[None,:,:,:]\n",
        "    result = model.predict(processed_image)\n",
        "    result = result.squeeze()\n",
        "    return result\n",
        "\n",
        "\n",
        "def plot_images_withLegends(image_number):\n",
        "  image = cv2.resize(cv2.cvtColor(cv2.imread(val_image_paths[image_number]), cv2.COLOR_BGR2RGB), dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
        "  mask_img = tf.io.read_file(val_mask_paths[image_number])\n",
        "  # mask = read_image(val_mask_paths[image_number])\n",
        "  mask = tf.image.decode_png(mask_img, channels=1)\n",
        "  mask.set_shape([None, None, 1])\n",
        "  mask = tf.image.resize(images=mask, size=[IMAGE_SIZE, IMAGE_SIZE], method='nearest')\n",
        "  one_hot_mask=to_categorical(mask,num_classes=NUM_CLASSES)\n",
        "  prediction=  predict(model, images, image_number, True)\n",
        "\n",
        "  colormap = plt.cm.get_cmap('viridis', len(class_labels))  # Use the number of unique classes\n",
        "  # Convert RGB to grayscale if needed\n",
        "  if len(mask.shape) == 3:\n",
        "      mask = np.mean(mask, axis=-1)\n",
        "\n",
        "  # Convert selected channel to grayscale\n",
        "  predicted_classes = prediction.argmax(axis=-1)\n",
        "\n",
        "# Get unique class indices present in the image\n",
        "  unique_classes = np.unique(predicted_classes)\n",
        "\n",
        "  # Convert to RGB using the colormap\n",
        "  rgb_image = colormap(predicted_classes)[:, :, :3]\n",
        "\n",
        "  # Convert RGB to grayscale\n",
        "  gray_image = np.mean(rgb_image, axis=-1)\n",
        "\n",
        "  # Display the result with dynamic legend\n",
        "  fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "  # Display the normal image with colors\n",
        "  axes[0].imshow(image, cmap='viridis')\n",
        "  axes[0].axis('off')\n",
        "  axes[0].set_title('Normal Image')\n",
        "\n",
        "  # Display the mask image as grayscale\n",
        "  axes[1].imshow(mask, cmap='gray')\n",
        "  axes[1].axis('off')\n",
        "  axes[1].set_title('Mask Image (Grayscale)')\n",
        "\n",
        "  # Display the prediction image for the selected class as grayscale\n",
        "  axes[2].imshow(gray_image, cmap='gray')\n",
        "  axes[2].axis('off')\n",
        "  axes[2].set_title(f'Prediction (Grayscale) for Class')\n",
        "\n",
        "  # Create a dynamic legend for unique classes in the prediction image\n",
        "  colormapLegend = plt.cm.get_cmap('gray', 104)\n",
        "  unique_classes = np.unique(predicted_classes)\n",
        "  # legend_elements = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', markersize=10, label=class_labels[int(j)]) for j in range(len(unique_classes))]\n",
        "  legend_elements = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colormapLegend(i)[:3], markersize=10, label=class_labels[i]) for i in unique_classes]\n",
        "  ax_inset = axes[2].inset_axes([1.05, 0, 0.2, 1])\n",
        "  ax_inset.legend(handles=legend_elements, loc='center', bbox_to_anchor=(0, 0.5))\n",
        "  ax_inset.axis('off')\n",
        "\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "zQ6X3OjBO8EV",
        "outputId": "1a21d7a2-2d47-4c26-a188-e35068f2d1f6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-c639928e90d2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_images_withLegends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-33bdbc5517d3>\u001b[0m in \u001b[0;36mplot_images_withLegends\u001b[0;34m(image_number)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mone_hot_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mcolormap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'viridis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use the number of unique classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predict' is not defined"
          ]
        }
      ],
      "source": [
        "plot_images_withLegends(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3L-s19mO7_5",
        "outputId": "40062960-e0f9-4f81-adab-dac38f1e8a4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall Maximum Pixel Value: 103\n"
          ]
        }
      ],
      "source": [
        " max_pixel_value = 0\n",
        "\n",
        " for image_path in mask_paths:\n",
        "  # Load and preprocess image\n",
        "  # img_img = tf.io.read_file(self.image_paths[i])\n",
        "  # img = tf.image.decode_png(img_img, channels=3)\n",
        "  # img.set_shape([None, None, 3])\n",
        "  # img = tf.image.resize(images=img, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "  # img = tf.keras.applications.resnet50.preprocess_input(img)\n",
        "  # batch_x.append(img)\n",
        "\n",
        "  # Load and preprocess mask\n",
        "  mask_img = tf.io.read_file(image_path)\n",
        "  mask = tf.image.decode_png(mask_img, channels=1)\n",
        "  mask.set_shape([None, None, 1])\n",
        "  mask = tf.image.resize(images=mask, size=[IMAGE_SIZE, IMAGE_SIZE], method='nearest')\n",
        "  max_value_in_image = np.max(mask)\n",
        "  max_pixel_value = max(max_pixel_value, max_value_in_image)\n",
        "  # batch_y.append(mask)\n",
        "\n",
        "print(\"Overall Maximum Pixel Value:\", max_pixel_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jna9zcc0O75b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qu6lGqdgFqbj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def load_and_preprocess_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image.set_shape([None, None, 3])\n",
        "    image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "    image = tf.keras.applications.resnet50.preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "##todo maybe the colors should be checked\n",
        "def load_and_preprocess_mask(mask_path):\n",
        "  image = tf.io.read_file(mask_path)\n",
        "  image = tf.image.decode_png(image, channels=1)\n",
        "  image.set_shape([None, None, 1])\n",
        "  image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE], method='nearest')\n",
        "  return image\n",
        "\n",
        "def load_and_preprocess_data(image_path, mask_path):\n",
        "    # Load and preprocess your images and masks\n",
        "    # You may need to resize, normalize, or apply other preprocessing steps\n",
        "    image = load_and_preprocess_image(image_path)\n",
        "    mask = load_and_preprocess_mask(mask_path)\n",
        "    return image, mask\n",
        "\n",
        "dataset = dataset.map(load_and_preprocess_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7wyFl7Xvq8Y"
      },
      "outputs": [],
      "source": [
        "##the model\n",
        "def convolution_block(\n",
        "    block_input,\n",
        "    num_filters=256,\n",
        "    kernel_size=3,\n",
        "    dilation_rate=1,\n",
        "    padding=\"same\",\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=dilation_rate,\n",
        "        padding=\"same\",\n",
        "        use_bias=use_bias,\n",
        "        kernel_initializer=keras.initializers.HeNormal(),\n",
        "    )(block_input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def DilatedSpatialPyramidPooling(dspp_input):\n",
        "    dims = dspp_input.shape\n",
        "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
        "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
        "    out_pool = layers.UpSampling2D(\n",
        "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "\n",
        "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
        "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
        "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
        "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
        "    output = convolution_block(x, kernel_size=1)\n",
        "    return output\n",
        "\n",
        "def DeeplabV3Plus(image_size, num_classes):\n",
        "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
        "    resnet50 = keras.applications.ResNet50(\n",
        "        weights=\"imagenet\", include_top=False, input_tensor=model_input\n",
        "    )\n",
        "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
        "    x = DilatedSpatialPyramidPooling(x)\n",
        "\n",
        "    input_a = layers.UpSampling2D(\n",
        "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
        "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
        "    x = convolution_block(x)\n",
        "    x = convolution_block(x)\n",
        "    x = layers.UpSampling2D(\n",
        "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\", kernel_initializer=keras.initializers.HeNormal())(x)\n",
        "    return keras.Model(inputs=model_input, outputs=model_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13WjhwHgTBZH"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(images['image']))\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "id": "eZ0yZo6TR0y5",
        "outputId": "80b6f170-ccd2-4ab8-8edf-424932f644ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-bf797d11fbca>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_iou_metric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 256, 256, 3), found shape=(256, 256, 3)\n"
          ]
        }
      ],
      "source": [
        "model = DeeplabV3Plus(IMAGE_SIZE, NUM_CLASSES)\n",
        "\n",
        "# Define a custom mIoU metric\n",
        "class MeanIoU(tf.keras.metrics.MeanIoU):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MeanIoU, self).__init__(num_classes=num_classes)\n",
        "\n",
        "mean_iou_metric = MeanIoU(NUM_CLASSES)\n",
        "\n",
        "# Compile the model with the custom metric\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy', mean_iou_metric])\n",
        "\n",
        "model.fit(train_dataset, steps_per_epoch=10, epochs=10,validation_data=val_dataset)\n",
        "model.evaluate(test_generator)\n",
        "\n",
        "# Access the mean IoU value\n",
        "mean_iou_value = mean_iou_metric.result().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "TtnY1pI1YAd7",
        "outputId": "ef577855-da7e-424b-859d-74a05decbfb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 0 images belonging to 0 classes.\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-3fba79febe1c>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Extract one batch for debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0msample_batch_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0msample_images_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_masks_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_batch_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batch_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-3fba79febe1c>\u001b[0m in \u001b[0;36mcustom_data_generator\u001b[0;34m(image_folder, mask_folder, batch_size, target_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     mask_generator = mask_datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mmask_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m         \"\"\"\n\u001b[0;32m-> 1649\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1650\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/FoodSeg103/Images/img_dir/masks'"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Path to your dataset\n",
        "dataset_path = Path(os.getcwd() + r\"/gdrive/My Drive/FoodSeg103/Images/img_dir\")\n",
        "\n",
        "def custom_data_generator(image_folder, mask_folder, batch_size=32, target_size=(256, 256)):\n",
        "    image_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    mask_datagen = ImageDataGenerator()\n",
        "\n",
        "    image_generator = image_datagen.flow_from_directory(\n",
        "        image_folder,\n",
        "        class_mode=None,\n",
        "        target_size=target_size,\n",
        "        batch_size=batch_size,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    mask_generator = mask_datagen.flow_from_directory(\n",
        "        mask_folder,\n",
        "        class_mode=None,\n",
        "        target_size=target_size,\n",
        "        batch_size=batch_size,\n",
        "        seed=42,\n",
        "        color_mode='grayscale'\n",
        "    )\n",
        "\n",
        "    while True:\n",
        "        image_batch = image_generator.next()\n",
        "        mask_batch = mask_generator.next()\n",
        "        yield image_batch, mask_batch\n",
        "\n",
        "# Create a custom data generator for both train and test\n",
        "train_generator = custom_data_generator(os.path.join(dataset_path, 'train'), os.path.join(dataset_path, 'masks'))\n",
        "test_generator = custom_data_generator(os.path.join(dataset_path, 'test'), os.path.join(dataset_path, 'masks'))\n",
        "\n",
        "# Extract one batch for debugging\n",
        "sample_batch_train = next(train_generator)\n",
        "sample_images_train, sample_masks_train = sample_batch_train[0], sample_batch_train[1]\n",
        "\n",
        "sample_batch_test = next(test_generator)\n",
        "sample_images_test, sample_masks_test = sample_batch_test[0], sample_batch_test[1]\n",
        "\n",
        "# Now, the shape of sample_images_train/masks_train and sample_images_test/masks_test should be (32, 256, 256, 3) and (32, 256, 256, 1) respectively\n",
        "print(sample_images_train.shape, sample_masks_train.shape)\n",
        "print(sample_images_test.shape, sample_masks_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "rlukq-kETADm",
        "outputId": "f929ba96-2121-4e51-8004-1aa95b6b0045"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f52ade90c6c3>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create an ImageDataGenerator for images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mimage_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m image_generator = image_datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdataset_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Include both 'train' and 'test' folders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ImageDataGenerator.flow_from_directory() got an unexpected keyword argument 'dataset_path'"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Path to your dataset\n",
        "dataset_path = Path(os.getcwd() + r\"/gdrive/My Drive/FoodSeg103/Images/img_dir\")\n",
        "\n",
        "# Create an ImageDataGenerator for images\n",
        "image_datagen = ImageDataGenerator(rescale=1./255)\n",
        "image_generator = image_datagen.flow_from_directory(\n",
        "    dataset_path=,\n",
        "    classes=['train', 'test'],  # Include both 'train' and 'test' folders\n",
        "    class_mode=None,\n",
        "    target_size=(256, 256),  # Adjust the target size based on your model input size\n",
        "    batch_size=32,\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "5VcfG91Av0z6",
        "outputId": "e8f92687-ee48-4a11-ad53-286149663d77"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d74f1b51338e>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_model_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-d74f1b51338e>\u001b[0m in \u001b[0;36mtrain_model_new\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDeeplabV3Plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   model.compile(\n\u001b[1;32m      4\u001b[0m     optimizer=keras.optimizers.SGD(\n\u001b[1;32m      5\u001b[0m         \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWEIGHT_DECAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMOMENTUM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclipnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLIPNORM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DeeplabV3Plus' is not defined"
          ]
        }
      ],
      "source": [
        "def train_model_new():\n",
        "  model=DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n",
        "  model.compile(\n",
        "    optimizer=keras.optimizers.SGD(\n",
        "        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY, momentum=MOMENTUM, clipnorm=CLIPNORM\n",
        "    ),\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy', tf.keras.metrics.MeanIoU(num_classes=NUM_CLASSES)\n",
        "    ])\n",
        "  history = model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS)\n",
        "  return (model, history)\n",
        "\n",
        "t= train_model_new()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rlF96w-seBF"
      },
      "outputs": [],
      "source": [
        "images = load_images_combined(10,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJBeKApJvLH-"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(images['image']))\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)\n",
        "\n",
        "# Batch and shuffle the training dataset\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Batch the validation dataset\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDrIPt8Isywr"
      },
      "outputs": [],
      "source": [
        "singleMask=load_and_preprocess_mask(images['masks'][0])\n",
        "ar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "iRlL31hhtizF",
        "outputId": "616fd712-bb06-4b5b-c920-1b3f4b3eddd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image Shape: (384, 512, 1)\n",
            "Image Data Type: <dtype: 'float32'>\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "img_raw = tf.io.read_file(images['masks'][0])\n",
        "\n",
        "# Decode the image, specifying the channels (1 for grayscale)\n",
        "img_tensor = tf.io.decode_image(img_raw, channels=1, dtype=tf.uint8)\n",
        "\n",
        "# Convert the image to float32 and normalize it to the range [0, 1]\n",
        "img_tensor = tf.image.convert_image_dtype(img_tensor, tf.float32)\n",
        "img_tensor = img_tensor / 103.0  # Assuming the original range is [0, 103]\n",
        "\n",
        "# Print information about the loaded image\n",
        "print(\"Image Shape:\", img_tensor.shape)\n",
        "print(\"Image Data Type:\", img_tensor.dtype)\n",
        "\n",
        "# Display the image using matplotlib or any other plotting library\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(tf.squeeze(img_tensor), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wq-HT5g8ph9E"
      },
      "outputs": [],
      "source": [
        "##model\n",
        "def convolution_block(\n",
        "    block_input,\n",
        "    num_filters=256,\n",
        "    kernel_size=3,\n",
        "    dilation_rate=1,\n",
        "    padding=\"same\",\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=dilation_rate,\n",
        "        padding=\"same\",\n",
        "        use_bias=use_bias,\n",
        "        kernel_initializer=keras.initializers.HeNormal(),\n",
        "    )(block_input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def DilatedSpatialPyramidPooling(dspp_input):\n",
        "    dims = dspp_input.shape\n",
        "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
        "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
        "    out_pool = layers.UpSampling2D(\n",
        "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "\n",
        "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
        "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
        "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
        "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
        "    output = convolution_block(x, kernel_size=1)\n",
        "    return output\n",
        "\n",
        "def DeeplabV3Plus(image_size, num_classes):\n",
        "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
        "    resnet50 = keras.applications.ResNet50(\n",
        "        weights=\"imagenet\", include_top=False, input_tensor=model_input\n",
        "    )\n",
        "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
        "    x = DilatedSpatialPyramidPooling(x)\n",
        "\n",
        "    input_a = layers.UpSampling2D(\n",
        "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
        "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
        "    x = convolution_block(x)\n",
        "    x = convolution_block(x)\n",
        "    x = layers.UpSampling2D(\n",
        "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\", kernel_initializer=keras.initializers.HeNormal())(x)\n",
        "    return keras.Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "\n",
        "def trainModel(Epochs):\n",
        "    model = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n",
        "    model.compile(\n",
        "      optimizer=keras.optimizers.SGD(\n",
        "          learning_rate=0.002, weight_decay=0.0001, momentum=0.9, clipnorm=10.0\n",
        "      ),\n",
        "      loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "      metrics=[\n",
        "          keras.metrics.MeanIoU(\n",
        "              num_classes=NUM_CLASSES, sparse_y_true=False, sparse_y_pred=False\n",
        "          ),\n",
        "          keras.metrics.CategoricalAccuracy(),\n",
        "      ],)\n",
        "    history = model.fit(train_dataset, validation_data=val_dataset, epochs=Epochs)\n",
        "    return (model, history)\n",
        "\n",
        "\n",
        "def trainNewModel(Epochs):\n",
        "    #Train new model and history\n",
        "    model = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False, ignore_class=None)\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.002),\n",
        "        loss=loss,\n",
        "        # metrics=[keras.metrics.MeanIoU(\n",
        "        #     num_classes=NUM_CLASSES, sparse_y_true=False, sparse_y_pred=False\n",
        "        # ),\n",
        "        # keras.metrics.CategoricalAccuracy(),],\n",
        "        metrics=[tf.keras.metrics.OneHotMeanIoU(num_classes=NUM_CLASSES,ignore_class=0)],\n",
        "        # metrics=[\"accuracy\"],\n",
        "    )\n",
        "    history = model.fit(train_dataset, validation_data=val_dataset, epochs=Epochs)\n",
        "    return (model, history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "DMOniNFGpRzY",
        "outputId": "9d42c41b-9348-4983-efc9-ad7829dc9e02"
      },
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-7648ccb5ff96>\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    def load_and_preprocess_data(image_path, mask_path):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 30\n"
          ]
        }
      ],
      "source": [
        "def train_model_mean_iou(Epochs):\n",
        "    model = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n",
        "    model.compile(\n",
        "      optimizer=keras.optimizers.SGD(\n",
        "          learning_rate=0.002, weight_decay=0.0001, momentum=0.9, clipnorm=10.0\n",
        "      ),\n",
        "      loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "      metrics=[\n",
        "          keras.metrics.MeanIoU(\n",
        "              num_classes=NUM_CLASSES, sparse_y_true=False, sparse_y_pred=False\n",
        "          ),\n",
        "          keras.metrics.CategoricalAccuracy(),\n",
        "      ],)\n",
        "    history = model.fit(train_dataset, validation_data=val_dataset, epochs=Epochs)\n",
        "    return (model, history)\n",
        "\n",
        "\n",
        "## load data in new manner\n",
        "#loading array with image paths\n",
        "image_paths, mask_paths = load_images_combined(100, 10)\n",
        "\n",
        "def load_and_preprocess_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image.set_shape([None, None, 3])\n",
        "    image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "    image = tf.keras.applications.resnet50.preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "def load_and_preprocess_mask(mask_path):\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(image_path, mask_path):\n",
        "    # Load and preprocess your images and masks\n",
        "    # You may need to resize, normalize, or apply other preprocessing steps\n",
        "    image = load_and_preprocess_image(image_path)\n",
        "    mask = load_and_preprocess_mask(mask_path)\n",
        "    return image, mask\n",
        "\n",
        "# def load_and_preprocess_image(image_path):\n",
        "#     # Implement image loading and preprocessing (e.g., resizing)\n",
        "#     # Return the preprocessed image as a NumPy array\n",
        "#     return processed_image\n",
        "\n",
        "def load_and_preprocess_mask(mask_path):\n",
        "    # Implement mask loading and preprocessing (e.g., resizing)\n",
        "    # Return the preprocessed mask as a NumPy array\n",
        "    return processed_mask\n",
        "\n",
        "# Load and preprocess the data\n",
        "data = [load_and_preprocess_data(image_path, mask_path) for image_path, mask_path in zip(image_paths, mask_paths)]\n",
        "images, masks = zip(*data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHFCaUa9pRSC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIoav0kQpBvQ",
        "outputId": "fdb4c42e-2056-4482-db70-f081a8347fd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input the folder of the model to load (the folder should be in the models folder and include a history.npy file and a model.keras file):\n"
          ]
        }
      ],
      "source": [
        "##loading model\n",
        "def loadModel(ModelPath):\n",
        "    print(\"Input the folder of the model to load (the folder should be in the models folder and include a history.npy file and a model.keras file):\")\n",
        "    # load_name = input()\n",
        "    folder_load_path = os.path.join(models_path, Path(ModelPath))\n",
        "    his_load_path = os.path.join(folder_load_path, Path(r\"history.npy\"))\n",
        "    model_load_path = os.path.join(folder_load_path, Path(r\"model.keras\"))\n",
        "    history=np.load(his_load_path,allow_pickle='TRUE').item()\n",
        "    model = keras.models.load_model(model_load_path)\n",
        "    return (model, history)\n",
        "\n",
        "model, history = loadModel(\"Base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfxk88tYpCDw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
